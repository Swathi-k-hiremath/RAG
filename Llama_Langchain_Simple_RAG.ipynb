{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "model_name = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
    "gen_cfg.max_new_tokens=512\n",
    "gen_cfg.temperature=0.0000001 # 0.0\n",
    "gen_cfg.return_full_text=True\n",
    "gen_cfg.do_sample=True\n",
    "gen_cfg.repetition_penalty=1.11\n",
    "\n",
    "pipe=pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=gen_cfg\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952891a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import fill\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "[INST] <>\n",
    "You are an AI assistant. You are truthful, unbiased and honest in your response.\n",
    "\n",
    "If you are unsure about an answer, truthfully say \"I don't know\"\n",
    "<>\n",
    "\n",
    "{text} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "text = \"Explain artificial intelligence in a few lines\"\n",
    "result = llm.invoke(prompt.format(text=text))\n",
    "print(fill(result.strip(), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
    "\n",
    "pdf_loader = UnstructuredPDFLoader(\"PDF_context_file.pdf\")\n",
    "pdf_doc = pdf_loader.load()\n",
    "updated_pdf_doc = filter_complex_metadata(pdf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed47e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Sotu-Biden-text.txt\", \"r\")\n",
    "content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "#chunked_pdf_doc = text_splitter.split_documents(updated_pdf_doc)\n",
    "chunked_pdf_doc = text_splitter.create_documents([content])\n",
    "len(chunked_pdf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d87847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.vectorstores import FAISS\n",
    "#db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ee88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "db_pdf = Chroma.from_documents(chunked_pdf_doc, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbe208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "[INST] <>\n",
    "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
    "\n",
    "<>\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "Chain_pdf = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    # retriever=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.8})\n",
    "    # Similarity Search is the default way to retrieve documents relevant to a query, but we can use MMR by setting search_type = \"mmr\"\n",
    "    # k defines how many documents are returned; defaults to 4.\n",
    "    # score_threshold allows to set a minimum relevance for documents returned by the retriever, if we are using the \"similarity_score_threshold\" search type.\n",
    "    # return_source_documents=True, # Optional parameter, returns the source documents used to answer the question\n",
    "    retriever=db_pdf.as_retriever(), # (search_kwargs={'k': 5, 'score_threshold': 0.8}),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "query = \"When was the solar system formed?\"\n",
    "result = Chain_pdf.invoke(query)\n",
    "print(fill(result['result'].strip(), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain in detail how the solar system was formed.\"\n",
    "result = Chain_pdf.invoke(query)\n",
    "print(fill(result['result'].strip(), width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac82b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the planets of the solar system composed of? Give a detailed response.\"\n",
    "result = Chain_pdf.invoke(query)\n",
    "print(fill(result['result'].strip(), width=100))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0e63db0",
   "metadata": {},
   "source": [
    "Hallucination check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca57762",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does the tranformers architecture work?\"\n",
    "result = Chain_pdf.invoke(query)\n",
    "print(fill(result['result'].strip(), width=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
